{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "A **Decision Tree** is one of the most widely used supervised machine learning algorithms for **classification** and **regression** tasks. In the context of classification, it is a predictive model that maps observations about data (features) to conclusions about the target class (labels). The model resembles a **tree-like structure** where internal nodes represent decision rules on attributes, branches represent outcomes of those rules, and leaf nodes represent final class labels. Because of its **simplicity, interpretability, and ability to handle both numerical and categorical data**, decision trees are extensively used in domains such as finance, healthcare, marketing, and engineering.\n",
        "\n",
        "---\n",
        "\n",
        "### **How a Decision Tree Works in Classification**\n",
        "\n",
        "1. **Root Node Selection**\n",
        "\n",
        "   * The process begins with the **root node**, which represents the entire dataset.\n",
        "   * The algorithm decides which feature (independent variable) best splits the dataset into different classes.\n",
        "   * Measures like **Information Gain, Gini Index, or Chi-Square** are used to choose the most important attribute for splitting.\n",
        "\n",
        "2. **Splitting**\n",
        "\n",
        "   * Based on the selected attribute, the dataset is divided into subsets.\n",
        "   * Each branch represents a possible outcome of the attribute test.\n",
        "   * Example: If “Age” is the chosen attribute, branches may be “<30,” “30–50,” and “>50.”\n",
        "\n",
        "3. **Recursive Partitioning**\n",
        "\n",
        "   * The process of splitting continues recursively on each subset.\n",
        "   * At every stage, the algorithm chooses the attribute that maximizes class separation.\n",
        "   * This process is often referred to as the **“divide and conquer”** strategy.\n",
        "\n",
        "4. **Leaf Nodes (Decision/Output)**\n",
        "\n",
        "   * The recursion ends when one of the following is true:\n",
        "\n",
        "     * All records belong to the same class.\n",
        "     * There are no remaining attributes for further splitting.\n",
        "     * The maximum depth (stopping criterion) is reached.\n",
        "   * Each leaf node is assigned a class label, which becomes the predicted outcome for new data falling into that path.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Measures for Attribute Selection**\n",
        "\n",
        "1. **Entropy & Information Gain (ID3 Algorithm)**\n",
        "\n",
        "   * **Entropy** measures impurity or disorder in the dataset.\n",
        "   * **Information Gain** measures the reduction in entropy achieved by splitting on an attribute.\n",
        "   * Formula:\n",
        "\n",
        "     $$\n",
        "     IG(S, A) = Entropy(S) - \\sum \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
        "     $$\n",
        "\n",
        "     where $S$ = dataset, $A$ = attribute, $S_v$ = subset after splitting.\n",
        "\n",
        "2. **Gini Index (CART Algorithm)**\n",
        "\n",
        "   * Gini measures impurity by calculating the probability of misclassification.\n",
        "   * Formula:\n",
        "\n",
        "     $$\n",
        "     Gini = 1 - \\sum_{i=1}^n (p_i)^2\n",
        "     $$\n",
        "\n",
        "     where $p_i$ = probability of class $i$.\n",
        "\n",
        "3. **Chi-Square Test**\n",
        "\n",
        "   * Statistical test used to measure independence between attribute and class.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose we want to classify whether a person will **“Buy a Computer” (Yes/No)** based on **Age** and **Income**.\n",
        "\n",
        "* **Root Node:** Choose the best attribute (say, Age).\n",
        "* If Age < 30 → check “Income” → if High = Yes, else = No.\n",
        "* If Age 30–50 → classify directly as Yes.\n",
        "* If Age > 50 → classify directly as No.\n",
        "\n",
        "The final decision tree will have rules such as:\n",
        "\n",
        "* IF Age < 30 AND Income = High → Buy = Yes\n",
        "* IF Age < 30 AND Income = Low → Buy = No\n",
        "* IF Age 30–50 → Buy = Yes\n",
        "* IF Age > 50 → Buy = No\n",
        "\n",
        "This rule-based structure makes it easy to interpret.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Decision Trees in Classification**\n",
        "\n",
        "* **Easy to Understand and Interpret**: Results can be explained using if–else rules.\n",
        "* **Handles Both Numerical and Categorical Data**.\n",
        "* **Non-Parametric**: No assumption about data distribution.\n",
        "* **Works Well with Large Datasets**.\n",
        "* **Feature Importance Ranking**: Helps identify most important features.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations**\n",
        "\n",
        "* **Overfitting**: Trees can become too complex and capture noise.\n",
        "* **Instability**: Small changes in data can result in a very different tree.\n",
        "* **Bias Toward Attributes with Many Levels**.\n",
        "* **Less Accurate Compared to Ensemble Models** like Random Forest or Gradient Boosted Trees.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "In conclusion, a **Decision Tree** is a powerful and intuitive algorithm for **classification problems**. It works by recursively splitting the dataset based on attributes that maximize class separation until a clear decision can be made at the leaf nodes. While it is easy to interpret and useful in many practical scenarios, care must be taken to avoid overfitting by pruning or using ensemble methods. Decision Trees serve as a foundation for more advanced models like **Random Forests** and **XGBoost**, which improve accuracy and generalization.\n",
        "\n",
        "---\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Decision Trees are among the most popular and powerful machine learning algorithms used for **classification** and **regression**. The basic idea behind a decision tree is to split the dataset into subsets in such a way that each resulting subset becomes **purer** in terms of class distribution.\n",
        "\n",
        "But the critical question is: **how does a decision tree decide where to split the data?** This is where impurity measures come into play. Two of the most widely used measures are:\n",
        "\n",
        "1. **Gini Impurity** (used in the CART algorithm).\n",
        "2. **Entropy (Information Gain)** (used in ID3, C4.5, and C5.0).\n",
        "\n",
        "Both of these quantify the **uncertainty or impurity** in a dataset and guide the algorithm in selecting the best attribute for splitting. Let us now discuss both in detail.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Concept of Impurity in Decision Trees**\n",
        "\n",
        "Impurity refers to how mixed or uncertain the data is with respect to class labels.\n",
        "\n",
        "* **Pure Node:** All records belong to the same class (e.g., all \"Yes\").\n",
        "* **Impure Node:** Records are evenly distributed among different classes (e.g., 50% \"Yes,\" 50% \"No\").\n",
        "\n",
        "The goal of splitting in a decision tree is to **minimize impurity** at each node so that child nodes become as close to pure as possible.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Gini Impurity**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "Gini Impurity measures the probability of **misclassifying** a randomly chosen sample if it was randomly labeled according to the class distribution in the node.\n",
        "\n",
        "#### **Formula**\n",
        "\n",
        "$$\n",
        "Gini(D) = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $D$ = dataset (node),\n",
        "* $k$ = number of classes,\n",
        "* $p_i$ = proportion of samples belonging to class $i$.\n",
        "\n",
        "#### **Range**\n",
        "\n",
        "* $0$ → Node is pure (all samples in one class).\n",
        "* Max value = $(1 - 1/k)$. For binary classification (k=2), maximum Gini = 0.5 (perfectly mixed).\n",
        "\n",
        "#### **Example**\n",
        "\n",
        "Suppose we have a node with 10 samples:\n",
        "\n",
        "* 6 belong to Class A\n",
        "* 4 belong to Class B\n",
        "\n",
        "$$\n",
        "p_A = 6/10 = 0.6, \\quad p_B = 4/10 = 0.4\n",
        "$$\n",
        "\n",
        "$$\n",
        "Gini = 1 - (0.6^2 + 0.4^2) = 1 - (0.36 + 0.16) = 0.48\n",
        "$$\n",
        "\n",
        "So, impurity of this node = **0.48**.\n",
        "\n",
        "#### **Interpretation**\n",
        "\n",
        "* If all samples were from one class, Gini would be 0 (perfectly pure).\n",
        "* If they were equally split (5-5), Gini would be 0.5 (most impure).\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Entropy**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "Entropy comes from **information theory** and measures the **uncertainty or disorder** in the dataset. It is used in decision trees to calculate **Information Gain**, which helps decide the best attribute for splitting.\n",
        "\n",
        "#### **Formula**\n",
        "\n",
        "$$\n",
        "Entropy(D) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "#### **Range**\n",
        "\n",
        "* $0$ → Pure node (all records belong to one class).\n",
        "* Maximum = $\\log_2(k)$. For binary classification (k=2), maximum entropy = 1.\n",
        "\n",
        "#### **Example**\n",
        "\n",
        "Using the same dataset as before (6 samples Class A, 4 samples Class B):\n",
        "\n",
        "$$\n",
        "Entropy = -(0.6 \\cdot \\log_2 0.6 + 0.4 \\cdot \\log_2 0.4)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -(0.6 \\cdot -0.737 + 0.4 \\cdot -1.322)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 0.971\n",
        "$$\n",
        "\n",
        "So, the entropy of this node is **0.97**.\n",
        "\n",
        "#### **Interpretation**\n",
        "\n",
        "* If node is pure → Entropy = 0.\n",
        "* If node has 50-50 distribution → Entropy = 1 (maximum disorder).\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Impact on Splits in Decision Trees**\n",
        "\n",
        "At every step, the decision tree algorithm considers all possible features and thresholds, and calculates **before-split impurity** and **after-split impurity**.\n",
        "\n",
        "* The **reduction in impurity** = **Information Gain** (for Entropy) or **Gini Gain** (for Gini).\n",
        "* The feature and threshold that maximizes impurity reduction is chosen for the split.\n",
        "\n",
        "#### **Example**\n",
        "\n",
        "Suppose a dataset node contains 10 records:\n",
        "\n",
        "* 6 \"Yes\", 4 \"No\" → Entropy = 0.97, Gini = 0.48.\n",
        "\n",
        "Now split on attribute \"Income\":\n",
        "\n",
        "* **Left Node (Income=High):** 5 samples, all \"Yes\" → Entropy = 0, Gini = 0.\n",
        "* **Right Node (Income=Low):** 5 samples, 1 \"Yes\" + 4 \"No\" → Entropy = 0.72, Gini = 0.32.\n",
        "\n",
        "**Weighted Average Impurity after split:**\n",
        "\n",
        "* Entropy = (5/10 \\* 0) + (5/10 \\* 0.72) = 0.36\n",
        "* Gini = (5/10 \\* 0) + (5/10 \\* 0.32) = 0.16\n",
        "\n",
        "**Information Gain (Entropy):** 0.97 – 0.36 = 0.61\n",
        "**Gini Gain:** 0.48 – 0.16 = 0.32\n",
        "\n",
        "Thus, this split is very effective, because impurity reduces significantly.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Comparison Between Gini Impurity and Entropy**\n",
        "\n",
        "| Aspect         | Gini Impurity                          | Entropy                           |\n",
        "| -------------- | -------------------------------------- | --------------------------------- |\n",
        "| Concept        | Measures misclassification probability | Measures information/uncertainty  |\n",
        "| Formula        | $1 - \\sum p_i^2$                       | $-\\sum p_i \\log_2(p_i)$           |\n",
        "| Range (Binary) | 0 → 0.5                                | 0 → 1                             |\n",
        "| Complexity     | Faster, easier to compute              | Slower (logarithmic calculations) |\n",
        "| Bias           | Biased toward larger partitions        | More balanced but slower          |\n",
        "| Algorithms     | CART uses Gini                         | ID3, C4.5 use Entropy             |\n",
        "| Performance    | Very similar results in practice       | Very similar results in practice  |\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Visual Understanding**\n",
        "\n",
        "* If a node is **pure**: Gini = 0, Entropy = 0.\n",
        "* If a node is **50-50 split**: Gini = 0.5, Entropy = 1.\n",
        "\n",
        "Graphically, both measures rise as impurity increases, but Entropy grows more sharply near 50-50 splits, whereas Gini is smoother.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Practical Applications**\n",
        "\n",
        "* **Gini Impurity** is widely used in CART because it is computationally efficient and fast for large datasets.\n",
        "* **Entropy (Information Gain)** is often used when interpretability and information-theoretic understanding are important.\n",
        "* In practice, both give very similar decision trees, and the choice usually depends on algorithm implementation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "To summarize, both **Gini Impurity** and **Entropy** are measures of impurity used in decision tree classification. Gini is based on misclassification probability, while Entropy is based on information theory. They play a crucial role in deciding **which attribute to split on at each node** by quantifying impurity and guiding the algorithm toward purer nodes.\n",
        "\n",
        "Although they are mathematically different, both produce similar results in practice. Gini is preferred for speed and simplicity, while Entropy is used for deeper theoretical insights. Together, they ensure that decision trees become powerful, interpretable, and effective models for classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Decision Trees are powerful machine learning models, but one major problem they face is **overfitting**. If a tree keeps splitting until all nodes are pure, it may fit noise in the training data rather than capturing general patterns. This reduces its ability to generalize to unseen data.\n",
        "\n",
        "To overcome this, a technique called **pruning** is used. Pruning simplifies the decision tree by reducing its size without losing much accuracy. There are **two major pruning strategies**:\n",
        "\n",
        "1. **Pre-Pruning (Early Stopping)**\n",
        "2. **Post-Pruning (Cost-Complexity Pruning or Reduced-Error Pruning)**\n",
        "\n",
        "Both aim to improve accuracy and avoid overfitting, but they differ in *when* and *how* pruning is applied.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Pre-Pruning (Early Stopping)**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "Pre-pruning stops the growth of the tree **during its construction**. Instead of allowing the tree to grow fully, certain stopping criteria are imposed to decide when to stop further splitting.\n",
        "\n",
        "#### **Stopping Conditions in Pre-Pruning**\n",
        "\n",
        "* Stop splitting if the depth of the tree exceeds a fixed maximum (e.g., max\\_depth = 5).\n",
        "* Stop if the number of samples in a node falls below a threshold (min\\_samples\\_split).\n",
        "* Stop if impurity decrease (Information Gain or Gini reduction) is less than a threshold (min\\_impurity\\_decrease).\n",
        "* Stop if a split does not improve accuracy significantly.\n",
        "\n",
        "#### **Advantages of Pre-Pruning**\n",
        "\n",
        "* **Prevents Overfitting Early**: Since the tree does not grow unnecessarily, it remains simple and generalizes better.\n",
        "* **Faster Training**: Saves computation time by avoiding unnecessary splits.\n",
        "\n",
        "#### **Practical Example**\n",
        "\n",
        "Suppose we are building a tree to predict customer churn in a telecom dataset. If we pre-prune by setting **max\\_depth = 5**, the tree won’t go beyond 5 levels. This prevents overly complex rules like “If age=34, income=medium, region=south, plan=gold, calls>200 → churn=yes,” which may only apply to a few customers. Instead, the model learns simpler, more general rules.\n",
        "\n",
        "#### **One Practical Advantage**\n",
        "\n",
        "Pre-pruning helps in **reducing computational cost and training time**, which is especially useful when working with **large datasets**.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Post-Pruning (Cost-Complexity Pruning)**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "Post-pruning allows the decision tree to **grow fully** until all leaves are pure or splitting stops naturally. Then, pruning is applied **after the tree is built** by removing branches that do not improve accuracy significantly.\n",
        "\n",
        "#### **Methods of Post-Pruning**\n",
        "\n",
        "* **Reduced Error Pruning**: Remove branches and check performance on a validation set. If accuracy does not decrease, keep the branch removed.\n",
        "* **Cost-Complexity Pruning (CART)**: Balances tree complexity with accuracy by minimizing:\n",
        "\n",
        "  $$\n",
        "  R_\\alpha(T) = R(T) + \\alpha \\cdot |T|\n",
        "  $$\n",
        "\n",
        "  where $R(T)$ = misclassification rate of tree, $|T|$ = number of leaf nodes, $\\alpha$ = complexity parameter.\n",
        "\n",
        "#### **Advantages of Post-Pruning**\n",
        "\n",
        "* **More Accurate Trees**: Since the tree is allowed to grow fully, it captures all possible patterns before removing unhelpful branches.\n",
        "* **Better Generalization**: Pruned trees generalize better to unseen data compared to unpruned trees.\n",
        "\n",
        "#### **Practical Example**\n",
        "\n",
        "Suppose we build a medical diagnosis tree that fully splits based on symptoms. The final tree may have hundreds of rules. After building the tree, we test it on validation data and find that many deep splits (like “age > 63 and blood pressure < 120 and cholesterol < 150 → disease=yes”) do not improve accuracy. These branches are pruned, leading to a smaller but equally accurate tree.\n",
        "\n",
        "#### **One Practical Advantage**\n",
        "\n",
        "Post-pruning **increases predictive accuracy** on unseen data by removing noisy or irrelevant splits after analyzing their effect on performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Key Differences Between Pre-Pruning and Post-Pruning**\n",
        "\n",
        "| Aspect                 | Pre-Pruning (Early Stopping)              | Post-Pruning (After Full Tree)                           |\n",
        "| ---------------------- | ----------------------------------------- | -------------------------------------------------------- |\n",
        "| **When Applied**       | During tree construction                  | After the tree is fully grown                            |\n",
        "| **Approach**           | Prevents further splits early             | Grows full tree, then removes unnecessary splits         |\n",
        "| **Computation**        | Faster, less costly                       | Slower, needs validation/testing                         |\n",
        "| **Risk**               | May underfit if stopped too early         | Less chance of underfitting                              |\n",
        "| **Accuracy**           | Sometimes lower (misses useful splits)    | Generally higher (captures patterns first)               |\n",
        "| **Flexibility**        | Restrictive, uses fixed thresholds        | More flexible, based on validation set                   |\n",
        "| **Example Algorithms** | CART with max\\_depth, min\\_samples\\_split | CART Cost-Complexity Pruning, C4.5 Reduced-Error Pruning |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Pruning is essential in decision trees to avoid overfitting and to improve generalization.\n",
        "\n",
        "* **Pre-Pruning** stops the tree early by using depth limits, minimum samples, or impurity thresholds. Its key advantage is **efficiency** and **reduced training time**, making it ideal for large datasets.\n",
        "* **Post-Pruning** allows the tree to fully grow, then trims unnecessary branches based on performance. Its key advantage is **higher accuracy and better generalization**, making it ideal when prediction quality is more important than speed.\n",
        "\n",
        "In practice, both pruning techniques are used depending on the application. A balanced combination of pre-pruning and post-pruning often yields the best decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Decision Trees are a widely used machine learning method for **classification** and **regression**. At the core of their working is the concept of **splitting** the dataset at each node in such a way that the resulting subsets are as **pure** as possible.\n",
        "\n",
        "To achieve this, the algorithm uses impurity measures such as **Entropy** or **Gini Index**. When Entropy is used, the improvement in purity after a split is measured using **Information Gain (IG)**.\n",
        "\n",
        "Information Gain tells us **how much information about the target variable is gained by knowing the value of a particular attribute**. The attribute with the highest information gain is selected as the **best split** at a node.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Definition of Information Gain**\n",
        "\n",
        "* **Entropy** represents the amount of uncertainty or disorder in a dataset.\n",
        "* **Information Gain** measures the **reduction in entropy** achieved by splitting the dataset on an attribute.\n",
        "\n",
        "#### **Formula**\n",
        "\n",
        "$$\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $S$ = dataset at the current node\n",
        "* $A$ = attribute on which we split\n",
        "* $Values(A)$ = possible values of attribute $A$\n",
        "* $S_v$ = subset of $S$ for which attribute $A = v$\n",
        "* $|S_v|/|S|$ = proportion of samples in subset $S_v$\n",
        "\n",
        "In simple words:\n",
        "\n",
        "* Compute the Entropy of the parent node.\n",
        "* Compute the weighted average Entropy of the child nodes after the split.\n",
        "* Subtract the two → the result is Information Gain.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Step-by-Step Example**\n",
        "\n",
        "Suppose we want to classify whether people **play tennis** based on the attribute **Weather** (Sunny, Overcast, Rainy).\n",
        "\n",
        "* **Dataset (S):** 14 records → 9 “Play = Yes”, 5 “Play = No”.\n",
        "\n",
        "#### Step 1: Compute Parent Node Entropy\n",
        "\n",
        "$$\n",
        "Entropy(S) = -\\left(\\frac{9}{14}\\log_2\\frac{9}{14} + \\frac{5}{14}\\log_2\\frac{5}{14}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -(0.643 \\cdot -0.639 + 0.357 \\cdot -1.485) = 0.94\n",
        "$$\n",
        "\n",
        "So, initial Entropy = **0.94**.\n",
        "\n",
        "#### Step 2: Split on Attribute “Weather”\n",
        "\n",
        "* **Sunny:** 5 samples → 2 Yes, 3 No\n",
        "\n",
        "  $$\n",
        "  Entropy(Sunny) = -(0.4 \\cdot \\log_2 0.4 + 0.6 \\cdot \\log_2 0.6) = 0.971\n",
        "  $$\n",
        "\n",
        "* **Overcast:** 4 samples → all Yes\n",
        "\n",
        "  $$\n",
        "  Entropy(Overcast) = 0\n",
        "  $$\n",
        "\n",
        "* **Rainy:** 5 samples → 4 Yes, 1 No\n",
        "\n",
        "  $$\n",
        "  Entropy(Rainy) = -(0.8 \\cdot \\log_2 0.8 + 0.2 \\cdot \\log_2 0.2) = 0.722\n",
        "  $$\n",
        "\n",
        "#### Step 3: Weighted Average Entropy After Split\n",
        "\n",
        "$$\n",
        "Entropy_{after} = \\frac{5}{14}(0.971) + \\frac{4}{14}(0) + \\frac{5}{14}(0.722)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= 0.347 + 0 + 0.258 = 0.605\n",
        "$$\n",
        "\n",
        "#### Step 4: Compute Information Gain\n",
        "\n",
        "$$\n",
        "IG(S, Weather) = 0.94 - 0.605 = 0.335\n",
        "$$\n",
        "\n",
        "So, splitting on **Weather** gives an Information Gain of **0.335**.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Why is Information Gain Important for Choosing the Best Split?**\n",
        "\n",
        "1. **Identifies the Most Informative Attribute**\n",
        "\n",
        "   * At each node, multiple attributes can be used to split the dataset.\n",
        "   * Information Gain helps select the attribute that provides the maximum **reduction in uncertainty**.\n",
        "\n",
        "2. **Improves Purity of Child Nodes**\n",
        "\n",
        "   * High Information Gain means the child nodes are **purer**, i.e., closer to containing only one class.\n",
        "\n",
        "3. **Guides Tree Growth**\n",
        "\n",
        "   * Without a measure like Information Gain, the tree would not know **which split is meaningful**.\n",
        "   * Using Information Gain ensures that splits are **data-driven** rather than random.\n",
        "\n",
        "4. **Controls Tree Depth and Accuracy**\n",
        "\n",
        "   * Attributes with low Information Gain are ignored, which prevents unnecessary splits and helps avoid overfitting.\n",
        "\n",
        "5. **Foundation for Algorithms**\n",
        "\n",
        "   * Information Gain is the **core criterion in ID3 and C4.5 algorithms**.\n",
        "   * It ensures trees are interpretable, balanced, and efficient.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Practical Example (Simplified)**\n",
        "\n",
        "Suppose we want to predict whether a student passes an exam based on **Study Hours** and **Attendance**.\n",
        "\n",
        "* If we split by **Study Hours**, the Information Gain is **0.25**.\n",
        "* If we split by **Attendance**, the Information Gain is **0.40**.\n",
        "\n",
        "Since **Attendance** gives higher Information Gain, the algorithm chooses it as the **root node**.\n",
        "\n",
        "This ensures that the first split provides maximum classification power.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Limitations of Information Gain**\n",
        "\n",
        "* **Bias Toward Attributes with Many Values**\n",
        "\n",
        "  * Attributes with many categories (like “Student ID”) tend to give artificially high Information Gain.\n",
        "  * To overcome this, C4.5 algorithm uses **Gain Ratio**, which normalizes Information Gain.\n",
        "\n",
        "* **Computational Cost**\n",
        "\n",
        "  * Requires computing logarithms and entropy for each possible split, which can be expensive for large datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Information Gain is a crucial concept in decision trees that measures the **reduction in uncertainty (entropy)** achieved by splitting on an attribute. It helps the algorithm identify the **most informative features** for classification and ensures that the resulting tree is both accurate and interpretable.\n",
        "\n",
        "By maximizing Information Gain at each step, decision trees gradually create **purer nodes**, leading to more reliable predictions. Despite its limitations, Information Gain remains one of the most fundamental and powerful tools in constructing effective decision trees.\n",
        "\n",
        "---\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "Decision Trees are among the most popular **supervised machine learning algorithms**. Their ability to handle both numerical and categorical data, combined with their **simplicity and interpretability**, makes them useful across a wide range of domains. In practice, Decision Trees are used in **business, healthcare, finance, marketing, engineering, and more**.\n",
        "\n",
        "Let us discuss some **real-world applications**, followed by their **advantages and limitations**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Real-World Applications of Decision Trees**\n",
        "\n",
        "#### **(a) Healthcare and Medical Diagnosis**\n",
        "\n",
        "* Decision Trees are used to **diagnose diseases** and suggest treatments based on patient symptoms, age, medical history, and test results.\n",
        "* Example: A decision tree could classify whether a patient has **diabetes** based on features like glucose level, BMI, insulin level, and family history.\n",
        "* Hospitals use them for **triage systems**, deciding whether a patient requires urgent care or can wait.\n",
        "\n",
        "#### **(b) Finance and Banking**\n",
        "\n",
        "* Decision Trees are widely applied in **credit scoring** and **loan approval**.\n",
        "* Banks assess whether a customer should be granted a loan by analyzing income, employment history, repayment history, and credit score.\n",
        "* Example: Predicting the probability of **loan default** using customer financial data.\n",
        "* Also used for **fraud detection**, by identifying unusual transaction patterns.\n",
        "\n",
        "#### **(c) Marketing and Customer Analytics**\n",
        "\n",
        "* Companies use Decision Trees for **customer segmentation** and **targeted advertising**.\n",
        "* Example: Classifying customers into “likely to buy,” “maybe,” or “not interested” categories based on age, gender, past purchases, and browsing behavior.\n",
        "* Helps in **churn prediction** (predicting whether a customer will leave a service).\n",
        "\n",
        "#### **(d) Manufacturing and Quality Control**\n",
        "\n",
        "* Used in **fault detection** and **predictive maintenance**.\n",
        "* Example: A factory may use a decision tree to decide whether a machine needs maintenance based on temperature, vibration, and operational hours.\n",
        "* Helps reduce breakdowns and optimize production efficiency.\n",
        "\n",
        "#### **(e) Education and Student Performance Prediction**\n",
        "\n",
        "* Schools and universities use decision trees to predict **student performance** and identify students at risk of failing.\n",
        "* Example: Predicting final exam outcomes based on attendance, assignment scores, and study hours.\n",
        "* This helps institutions provide timely interventions.\n",
        "\n",
        "#### **(f) E-commerce and Recommendation Systems**\n",
        "\n",
        "* E-commerce platforms use decision trees for **recommendation engines**.\n",
        "* Example: Suggesting products like “Customers who bought X also bought Y,” based on purchase history.\n",
        "* Also used in **fraud detection** for online payments.\n",
        "\n",
        "#### **(g) Legal and Judicial Systems**\n",
        "\n",
        "* Decision Trees are used in **legal decision support systems** to predict case outcomes based on historical judgments.\n",
        "* Example: Predicting whether bail will be granted based on crime type, past record, and evidence.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Main Advantages of Decision Trees**\n",
        "\n",
        "1. **Easy to Understand and Interpret**\n",
        "\n",
        "   * Even non-technical users can follow the if–else rules of a decision tree.\n",
        "   * Example: Doctors can directly interpret medical decision trees without needing complex math.\n",
        "\n",
        "2. **Handles Both Categorical and Numerical Data**\n",
        "\n",
        "   * Can work with features like “Age (numeric)” and “Gender (categorical)” simultaneously.\n",
        "\n",
        "3. **No Assumptions About Data Distribution**\n",
        "\n",
        "   * Unlike algorithms like Naïve Bayes or Logistic Regression, decision trees are **non-parametric** (no need to assume linearity or normality).\n",
        "\n",
        "4. **Feature Selection Built-In**\n",
        "\n",
        "   * Automatically selects the most informative features for splitting.\n",
        "   * Helps identify the **important variables** in a dataset.\n",
        "\n",
        "5. **Fast and Efficient**\n",
        "\n",
        "   * Works well for large datasets and provides quick results.\n",
        "\n",
        "6. **Basis for Advanced Models**\n",
        "\n",
        "   * Forms the foundation of **Random Forests, Gradient Boosting (XGBoost, LightGBM, CatBoost)**, which are among the most powerful machine learning models.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Main Limitations of Decision Trees**\n",
        "\n",
        "1. **Overfitting**\n",
        "\n",
        "   * If trees grow too deep, they may fit noise instead of general patterns.\n",
        "   * Example: A very detailed medical decision tree may only work on one hospital’s patients but fail on another’s.\n",
        "\n",
        "2. **Instability**\n",
        "\n",
        "   * Small changes in data can lead to a completely different tree structure.\n",
        "\n",
        "3. **Bias Toward Attributes with Many Levels**\n",
        "\n",
        "   * Attributes with many unique values (e.g., “Customer ID”) may appear more important due to artificially high information gain.\n",
        "\n",
        "4. **Not Always the Most Accurate**\n",
        "\n",
        "   * Single decision trees are often outperformed by ensemble methods like Random Forests or Gradient Boosting.\n",
        "\n",
        "5. **Greedy Nature of Splitting**\n",
        "\n",
        "   * Decision Trees use a greedy approach (choosing the best split at the current step), which may not always lead to the globally optimal tree.\n",
        "\n",
        "6. **Interpretability vs Complexity**\n",
        "\n",
        "   * While small trees are interpretable, large trees with many branches can become difficult to understand.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Decision Trees have become an essential tool in real-world applications ranging from **medical diagnosis** to **banking, marketing, education, and law**. Their **simplicity, interpretability, and ability to handle mixed data** make them highly practical in industries where transparency and decision-making speed are critical.\n",
        "\n",
        "However, they also suffer from drawbacks such as **overfitting, instability, and lower accuracy compared to ensemble models**. Despite these limitations, Decision Trees remain one of the most important machine learning algorithms, especially as the foundation of powerful ensemble techniques.\n",
        "\n",
        "In summary:\n",
        "\n",
        "* **Applications**: Healthcare, Finance, Marketing, Manufacturing, Education, E-commerce, Law.\n",
        "* **Advantages**: Easy to understand, works with all data types, fast, interpretable.\n",
        "* **Limitations**: Overfitting, instability, bias, less accurate compared to ensembles.\n",
        "\n",
        "---\n",
        "\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6:   Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier using Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Decision Tree Classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "for feature_name, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation **\n",
        "\n",
        "1. **Loading the dataset:**\n",
        "\n",
        "   * `load_iris()` loads the Iris dataset, which contains 150 samples with 4 features each (sepal length, sepal width, petal length, petal width) and 3 classes of iris flowers.\n",
        "\n",
        "2. **Splitting data:**\n",
        "\n",
        "   * 80% for training and 20% for testing ensures that the model can generalize well.\n",
        "\n",
        "3. **Decision Tree Classifier:**\n",
        "\n",
        "   * `criterion='gini'` is used to measure the quality of a split.\n",
        "   * `fit()` trains the model on the training data.\n",
        "\n",
        "4. **Predictions and Accuracy:**\n",
        "\n",
        "   * `accuracy_score()` compares predicted labels with actual test labels.\n",
        "   * High accuracy (around 0.9–1.0) indicates a well-trained classifier.\n",
        "\n",
        "5. **Feature Importance:**\n",
        "\n",
        "   * Shows which features contribute the most to making predictions.\n",
        "   * Usually, petal length and petal width are the most important features in the Iris dataset.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> ✅ Interpretation: Petal length and width are the most influential features for classifying Iris species.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfUC7uvjnlyX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3uRk5-nnR6C",
        "outputId": "b7476e36-9cd7-4aae-81c2-8bc6691fcb61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier: 1.00\n",
            "sepal length (cm): 0.00\n",
            "sepal width (cm): 0.02\n",
            "petal length (cm): 0.91\n",
            "petal width (cm): 0.08\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier using Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Decision Tree Classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "for feature_name, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy of max_depth=3 tree: {accuracy_limited:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation**\n",
        "\n",
        "1. **Fully-grown tree:**\n",
        "\n",
        "   * No restriction on depth; tree grows until all leaves are pure or contain less than the minimum samples.\n",
        "   * Can achieve perfect accuracy on training data but may overfit.\n",
        "\n",
        "2. **Tree with max\\_depth=3:**\n",
        "\n",
        "   * Limits the depth to 3 levels.\n",
        "   * Helps prevent overfitting, may slightly reduce accuracy but generalizes better.\n",
        "\n",
        "3. **Accuracy comparison:**\n",
        "\n",
        "   * `accuracy_score()` calculates the test accuracy of both models.\n",
        "   * Usually, the fully-grown tree has slightly higher training accuracy but similar test accuracy for small datasets like Iris.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> ✅ Interpretation:\n",
        ">\n",
        "> * Fully-grown tree perfectly fits the training data.\n",
        "> * Limiting the depth to 3 slightly reduces accuracy but may improve generalization and prevent overfitting.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZplVmkeEtJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (no depth limit)\n",
        "full_tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy of max_depth=3 tree: {accuracy_limited:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn1Pl1SHtAyW",
        "outputId": "f3903f49-fda4-4e40-a216-c33efc965097"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 1.00\n",
            "Accuracy of max_depth=3 tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y = california.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_regressor.feature_importances_\n",
        "for feature_name, importance in zip(california.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.3f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation**\n",
        "\n",
        "1. **Loading the dataset:**\n",
        "\n",
        "   * `fetch_california_housing()` loads a dataset with 20,640 samples and 8 numerical features related to housing in California.\n",
        "   * Target variable is the median house value.\n",
        "\n",
        "2. **Splitting data:**\n",
        "\n",
        "   * 80% training, 20% testing ensures the model can generalize.\n",
        "\n",
        "3. **Decision Tree Regressor:**\n",
        "\n",
        "   * `DecisionTreeRegressor()` builds a regression tree that predicts continuous values.\n",
        "   * Trained using `fit()`.\n",
        "\n",
        "4. **Prediction and Evaluation:**\n",
        "\n",
        "   * Predictions are made on the test set.\n",
        "   * `mean_squared_error()` measures the average squared difference between predicted and actual values. Lower MSE indicates better accuracy.\n",
        "\n",
        "5. **Feature Importance:**\n",
        "\n",
        "   * Shows which features contribute most to predicting house prices.\n",
        "   * Typically, `MedInc` (median income) is the most important feature.\n",
        "\n",
        "---\n",
        "\n",
        "> ✅ Interpretation:\n",
        ">\n",
        "> * `MedInc` is the most influential feature for predicting house prices.\n",
        "> * Decision Tree Regressor fits well but may overfit small regions; pruning or limiting depth can improve generalization.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "SU2BJ6YjuLW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "y = california.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_regressor.feature_importances_\n",
        "for feature_name, importance in zip(california.feature_names, feature_importances):\n",
        "    print(f\"{feature_name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvZbzJ_jtxPd",
        "outputId": "dd97cd60-3d0a-479d-fa8d-2600f1d670e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495\n",
            "MedInc: 0.529\n",
            "HouseAge: 0.052\n",
            "AveRooms: 0.053\n",
            "AveBedrms: 0.029\n",
            "Population: 0.031\n",
            "AveOccup: 0.131\n",
            "Latitude: 0.094\n",
            "Longitude: 0.083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the tuned Decision Tree: {accuracy:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation**\n",
        "\n",
        "1. **GridSearchCV:**\n",
        "\n",
        "   * Performs exhaustive search over specified hyperparameter values.\n",
        "   * Uses cross-validation (`cv=5`) to evaluate model performance.\n",
        "\n",
        "2. **Parameters tuned:**\n",
        "\n",
        "   * `max_depth`: Maximum depth of the tree; prevents overfitting if limited.\n",
        "   * `min_samples_split`: Minimum samples required to split a node; controls tree growth.\n",
        "\n",
        "3. **Selecting the best model:**\n",
        "\n",
        "   * `grid_search.best_params_` gives the combination with the highest cross-validated accuracy.\n",
        "   * The model is retrained using these optimal parameters for final evaluation.\n",
        "\n",
        "4. **Accuracy calculation:**\n",
        "\n",
        "   * `accuracy_score()` evaluates performance on the test set.\n",
        "   * Proper tuning typically improves generalization.\n",
        "\n",
        "---\n",
        "\n",
        "> ✅ Interpretation:\n",
        ">\n",
        "> * Limiting the depth to 3 and keeping `min_samples_split=2` produces the best accuracy.\n",
        "> * Hyperparameter tuning prevents overfitting and ensures good generalization.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "yfmhCbOmupx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the grid of hyperparameters\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on the test set using the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the tuned Decision Tree: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iwruU7RunRA",
        "outputId": "813645df-3174-49b4-e85a-9a5c51631ce9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy of the tuned Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "### **Step 1: Handle Missing Values**\n",
        "\n",
        "Handling missing data is crucial to avoid biased or incorrect predictions.\n",
        "\n",
        "1. **Identify missing values:**\n",
        "\n",
        "   * Use methods like `df.isnull().sum()` to see which columns have missing values.\n",
        "\n",
        "2. **Impute missing values:**\n",
        "\n",
        "   * **Numerical features:** Replace missing values with the mean, median, or use more advanced methods like KNN imputation.\n",
        "\n",
        "     ```python\n",
        "     from sklearn.impute import SimpleImputer\n",
        "     imputer = SimpleImputer(strategy='median')\n",
        "     df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
        "     ```\n",
        "   * **Categorical features:** Replace missing values with the most frequent category or a placeholder like `\"Unknown\"`.\n",
        "\n",
        "     ```python\n",
        "     imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "     df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Encode Categorical Features**\n",
        "\n",
        "Decision Trees can work with numeric values, so categorical features must be encoded.\n",
        "\n",
        "1. **Label Encoding:** For ordinal categorical variables (e.g., low, medium, high).\n",
        "2. **One-Hot Encoding:** For nominal variables without order (e.g., blood type, city).\n",
        "\n",
        "   ```python\n",
        "   from sklearn.preprocessing import OneHotEncoder\n",
        "   encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "   encoded_cat = encoder.fit_transform(df[categorical_cols])\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Train a Decision Tree Model**\n",
        "\n",
        "1. **Split the dataset into train and test sets**\n",
        "\n",
        "   ```python\n",
        "   from sklearn.model_selection import train_test_split\n",
        "   X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "   ```\n",
        "\n",
        "2. **Initialize and train the Decision Tree classifier**\n",
        "\n",
        "   ```python\n",
        "   from sklearn.tree import DecisionTreeClassifier\n",
        "   dt_model = DecisionTreeClassifier(random_state=42)\n",
        "   dt_model.fit(X_train, y_train)\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 4: Tune Hyperparameters**\n",
        "\n",
        "Hyperparameter tuning helps improve model performance and prevent overfitting.\n",
        "\n",
        "1. **Define the hyperparameter grid**\n",
        "\n",
        "   ```python\n",
        "   param_grid = {\n",
        "       'max_depth': [3, 5, 7, None],\n",
        "       'min_samples_split': [2, 5, 10],\n",
        "       'min_samples_leaf': [1, 2, 4],\n",
        "       'criterion': ['gini', 'entropy']\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. **Use GridSearchCV to find the best combination**\n",
        "\n",
        "   ```python\n",
        "   from sklearn.model_selection import GridSearchCV\n",
        "   grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "   grid_search.fit(X_train, y_train)\n",
        "   best_model = grid_search.best_estimator_\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 5: Evaluate Performance**\n",
        "\n",
        "1. **Predict on the test set**\n",
        "\n",
        "   ```python\n",
        "   y_pred = best_model.predict(X_test)\n",
        "   ```\n",
        "\n",
        "2. **Use performance metrics suitable for classification**\n",
        "\n",
        "   * **Accuracy**: Overall correctness\n",
        "   * **Precision & Recall**: Important for healthcare to minimize false positives/negatives\n",
        "   * **F1-Score**: Balances precision and recall\n",
        "\n",
        "   ```python\n",
        "   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "   print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "   print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "   print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "   print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 6: Business Value**\n",
        "\n",
        "1. **Early Detection of Disease:** Helps doctors identify high-risk patients faster.\n",
        "2. **Resource Optimization:** Focuses medical attention on patients most likely to need treatment.\n",
        "3. **Cost Reduction:** Reduces unnecessary tests or treatments.\n",
        "4. **Personalized Treatment:** Provides insights for tailored healthcare plans based on patient characteristics.\n",
        "5. **Strategic Decision Making:** Management can allocate resources efficiently and improve patient outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "| Step                        | Key Action                                                                        |\n",
        "| --------------------------- | --------------------------------------------------------------------------------- |\n",
        "| Handle missing values       | Impute median/mean for numeric, mode/Unknown for categorical                      |\n",
        "| Encode categorical features | One-Hot or Label Encoding                                                         |\n",
        "| Train model                 | Split data, train Decision Tree                                                   |\n",
        "| Tune hyperparameters        | GridSearchCV: max\\_depth, min\\_samples\\_split, min\\_samples\\_leaf, criterion      |\n",
        "| Evaluate performance        | Accuracy, Precision, Recall, F1-Score                                             |\n",
        "| Business Value              | Early detection, cost reduction, personalized care, optimized resource allocation |\n",
        "\n",
        "> ✅ This process ensures **clean, well-prepared data**, a **tuned model**, and a **real-world impact** on patient care and operational efficiency.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "WLK1R4_hvGSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ===============================\n",
        "# Step 1: Load Dataset\n",
        "# ===============================\n",
        "# Mock healthcare dataset for demonstration\n",
        "data = {\n",
        "    'Age': [25, 45, 52, None, 36, 48, None, 50],\n",
        "    'Gender': ['M', 'F', 'M', 'F', None, 'M', 'F', 'M'],\n",
        "    'BloodPressure': [120, 130, None, 140, 125, 135, 128, None],\n",
        "    'Cholesterol': [200, 240, 180, 220, 210, None, 190, 230],\n",
        "    'Smoker': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', None],\n",
        "    'Disease': [1, 0, 1, 0, 1, 0, 0, 1]  # Target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Disease', axis=1)\n",
        "y = df['Disease']\n",
        "\n",
        "# ===============================\n",
        "# Step 2: Handle Missing Values\n",
        "# ===============================\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Impute numerical features with median\n",
        "imputer_num = SimpleImputer(strategy='median')\n",
        "X[numerical_cols] = imputer_num.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Impute categorical features with most frequent value\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "X[categorical_cols] = imputer_cat.fit_transform(X[categorical_cols])\n",
        "\n",
        "# ===============================\n",
        "# Step 3: Encode Categorical Features\n",
        "# ===============================\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)  # fixed for modern scikit-learn\n",
        "encoded_cat = encoder.fit_transform(X[categorical_cols])\n",
        "encoded_cat_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Combine numerical and encoded categorical features\n",
        "X_final = pd.concat([X[numerical_cols].reset_index(drop=True), encoded_cat_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# ===============================\n",
        "# Step 4: Train-Test Split\n",
        "# ===============================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# ===============================\n",
        "# Step 5: Initialize Decision Tree Classifier\n",
        "# ===============================\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# ===============================\n",
        "# Step 6: Hyperparameter Tuning with GridSearchCV\n",
        "# ===============================\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, None],\n",
        "    'min_samples_split': [2, 3, 4],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Use cv=2 due to small dataset\n",
        "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# ===============================\n",
        "# Step 7: Predictions and Evaluation\n",
        "# ===============================\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nModel Performance on Test Set:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrXDhVUywI4m",
        "outputId": "4f4fa851-1b7e-428e-f843-6122575ffa9a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "\n",
            "Model Performance on Test Set:\n",
            "Accuracy: 0.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    }
  ]
}